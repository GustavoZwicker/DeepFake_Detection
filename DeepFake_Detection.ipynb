{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mtcnn matplotlib face_recognition"
      ],
      "metadata": {
        "id": "5VdkAr_ja3fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib"
      ],
      "metadata": {
        "id": "LK4fbCuDf14Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "Qic39q5CCXBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu2NfOedAFlQ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "import imutils\n",
        "from tqdm import tqdm\n",
        "from mtcnn import MTCNN\n",
        "import tensorflow as tf\n",
        "import concurrent\n",
        "from skimage import data, img_as_float\n",
        "from skimage import exposure\n",
        "import face_recognition\n",
        "from skimage import img_as_ubyte\n",
        "from collections import Counter\n",
        "import json\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Read dataset Metadata\n",
        "name: File name <br>\n",
        "label: Fake | Real <br>\n",
        "split: Is on test or training set <br>\n",
        "original: If it's fake, original's file name counterpart."
      ],
      "metadata": {
        "id": "U9trvOuxCWaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(os.path.join('Dataset','metadata.json'), 'r')\n",
        "labels = json.load(f)\n",
        "real_videos = [x for x in labels.keys() if labels[x]['label'] == 'REAL']"
      ],
      "metadata": {
        "id": "Za2nvdItARGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(os.path.join('Dataset', 'bkzrsfcrcr.mp4'))\n",
        "ret, frame = cap.read()"
      ],
      "metadata": {
        "id": "QdB2Cp6DENxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "# GET Height\n",
        "print(f'Height: {cap.get(cv2.CAP_PROP_FRAME_HEIGHT)}')\n",
        "\n",
        "# GET Width\n",
        "print(f'Width: {cap.get(cv2.CAP_PROP_FRAME_WIDTH)}')\n",
        "\n",
        "# GET nº of Frames\n",
        "print(f'Nº of Frames: {cap.get(cv2.CAP_PROP_FRAME_COUNT)}')\n",
        "\n",
        "# GET FPS\n",
        "print(f'FPS: {cap.get(cv2.CAP_PROP_FPS)}')\n"
      ],
      "metadata": {
        "id": "IJ8LRH7oFGW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 - Crop only Face with Haarcascade\n",
        "\n",
        "It created a lot of inconsistencies"
      ],
      "metadata": {
        "id": "Lsc8vKs3NSl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.patches import Rectangle\n",
        "def create_color_hist(img):\n",
        "  channels = [Counter(img[:,:,i].reshape(-1)) for i in range(3)]\n",
        "  hist = {i:[channels[i][intensity] for intensity in range(256)] for i in range(3)}\n",
        "\n",
        "  return hist\n",
        "\n",
        "def crop_face_haar(cap, frame_skip):\n",
        "  # print('COMECEI')\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  p_frames = []\n",
        "  i = 0\n",
        "  for frame_i in range(0, frame_count, frame_skip):\n",
        "    ret, frame_original = cap.read()\n",
        "    frame_original = cv2.cvtColor(frame_original, cv2.COLOR_BGR2RGB)\n",
        "    frame = cv2.cvtColor(frame_original, cv2.COLOR_RGB2GRAY)\n",
        "    frame = exposure.equalize_hist(frame)\n",
        "    frame = img_as_ubyte(frame)\n",
        "    print(f'FRAME: {frame_i}')\n",
        "    if i > frame_skip:\n",
        "      faces = prev_face\n",
        "    else:\n",
        "      faces = face_recognition.face_locations(frame, model=\"hog\")\n",
        "    # print(faces)\n",
        "    if len(faces) >= 1:\n",
        "      prev_face = faces\n",
        "    else:\n",
        "      try:\n",
        "        faces = prev_face\n",
        "      except:\n",
        "        prev_face = None\n",
        "        i = 0\n",
        "        # while faces == None or (type(faces) == list and len(faces) == 0):\n",
        "        while prev_face == None or len(prev_face) == 0:\n",
        "          i += 1\n",
        "          # print(faces)\n",
        "          ret_next, frame_next = cap.read()\n",
        "          frame = cv2.cvtColor(frame_next, cv2.COLOR_RGB2GRAY)\n",
        "          frame = exposure.equalize_hist(frame)\n",
        "          faces = face_recognition.face_locations(frame_next, model=\"hog\")\n",
        "          prev_face = faces\n",
        "          if i >= 50:\n",
        "            return False\n",
        "    # print(faces)\n",
        "    # print(prev_face)\n",
        "    top, right, bottom, left = faces[0]\n",
        "    vertF = 300 - (bottom - top)\n",
        "    horF = 300 - (right - left)\n",
        "    topF, rightF, botF, leftF = top, right, bottom, left\n",
        "    if vertF != 0:\n",
        "      topF = max(0, top - vertF//2)\n",
        "      # print(topF)\n",
        "      botF = bottom + (vertF - vertF//2) - min(0, top - vertF//2)\n",
        "    if horF != 0:\n",
        "      leftF = max(0, left - horF//2)\n",
        "      rightF = right + (horF - (horF//2)) - min(0, left - horF//2)\n",
        "    p_frames.append(frame_original[topF:botF, leftF:rightF])\n",
        "    # print(len(frame_original[topF:botF, leftF:rightF]),len(frame_original[topF:botF, leftF:rightF][0]))\n",
        "    # print(f'{topF}:{botF} \\\\\\ {leftF}:{rightF}')\n",
        "    # plt.figure()\n",
        "    # plt.imshow(frame_original[topF:botF,leftF:rightF])\n",
        "    # plt.show()\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_i-1)\n",
        "    # xl, yl, wl, hl = x, y, w, h\n",
        "  return p_frames\n",
        "\n",
        "for path in glob.glob(os.path.join('Dataset', '*'))[740:]:\n",
        "  print(path)\n",
        "  video_name = path.split('/')[1]\n",
        "  if video_name in real_videos:\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    print(video_name)\n",
        "\n",
        "    dimensions = (300, 300)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    video_writer = cv2.VideoWriter(os.path.join('Output', 'processed_'+video_name), cv2.VideoWriter_fourcc(*'DIVX'), fps, dimensions, isColor=True)\n",
        "    # with tf.device('/device:GPU:0'):\n",
        "    video_frames = crop_face_haar(cap, 10)\n",
        "    # print(len(video_frames))\n",
        "    # print([len(face_frame) for face_frame in video_frames])\n",
        "    if video_frames != False:\n",
        "      [video_writer.write(cv2.cvtColor(face_frame, cv2.COLOR_RGB2BGR)) for face_frame in video_frames]\n",
        "      cap.release()\n",
        "      video_writer.release()\n",
        "    else:\n",
        "      file1 = open(\"video_errors.txt\", \"a\")  # append mode\n",
        "      file1.write(f\"{video_name} \\n\")\n",
        "      file1.close()"
      ],
      "metadata": {
        "id": "nIEUsLeJZLyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Append-adds at last\n",
        "file1 = open(\"myfile.txt\", \"a\")  # append mode\n",
        "file1.write(\"Today \\n\")\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "-AXzGvqUpl3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"myfile.txt\", \"x\")"
      ],
      "metadata": {
        "id": "FKV-biAipW8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "video_name = 'dzdbunuaag.mp4'\n",
        "\n",
        "cap = cv2.VideoCapture(os.path.join('Dataset', video_name))\n",
        "\n",
        "dimensions = (300, 300)\n",
        "\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "video_writer = cv2.VideoWriter(os.path.join('Output', 'face_'+video_name), cv2.VideoWriter_fourcc(*'DIVX'), fps, dimensions, isColor=False)\n",
        "\n",
        "# for frame_i in range(2):\n",
        "ret, frame = cap.read()\n",
        "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "# frame = cv2.convertScaleAbs(frame, alpha=4, beta=5)\n",
        "\n",
        "p2, p98 = np.percentile(frame, (2, 98))\n",
        "img_rescale = exposure.rescale_intensity(frame, in_range=(p2, p98))\n",
        "\n",
        "# Equalization\n",
        "img_eq = exposure.equalize_hist(frame)\n",
        "\n",
        "# Adaptive Equalization\n",
        "img_adapteq = exposure.equalize_adapthist(frame, clip_limit=0.03)\n",
        "\n",
        "  detector = cv2.FaceDetectorYN.create(\"Models/face_detection_yunet.onnx\",  \"\", (0, 0))\n",
        "  detector.setInputSize((width, height))\n",
        "\n",
        "  _, faces = detector.detect(frame)\n",
        "\n",
        "  # if faces[1] is None, no face found\n",
        "\n",
        "  if faces is not None:\n",
        "    for face in faces:\n",
        "        # parameters: x1, y1, w, h, x_re, y_re, x_le, y_le, x_nt, y_nt, x_rcm, y_rcm, x_lcm, y_lcm\n",
        "\n",
        "        # bouding box\n",
        "        box = list(map(int, face[:4]))\n",
        "        color = (0, 0, 255)\n",
        "        cv2.rectangle(frame, box, color, 5)\n",
        "\n",
        "        # confidence\n",
        "        confidence = face[-1]\n",
        "        confidence = \"{:.2f}\".format(confidence)\n",
        "        position = (box[0], box[1] - 10)\n",
        "        cv2.putText(frame, confidence, position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 3, cv2.LINE_AA)\n",
        "plt.figure()\n",
        "plt.imshow(frame)\n",
        "plt.figure()\n",
        "plt.imshow(img_rescale)\n",
        "plt.figure()\n",
        "plt.imshow(img_eq)\n",
        "plt.figure()\n",
        "plt.imshow(img_adapteq)"
      ],
      "metadata": {
        "id": "b3gL1kCJXJBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_cascade = cv2.CascadeClassifier('Models/haarcascade_frontalface_alt2.xml')\n",
        "frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "faces = face_cascade.detectMultiScale(frame, 1.1, 4)\n",
        "\n",
        "for (x, y, w, h) in faces:\n",
        "    # cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
        "    faces = frame[y:y + h, x:x + w]\n",
        "\n",
        "    # cv2_imshow(faces)\n",
        "    cv2_imshow(imutils.resize(faces, 350, 350))\n",
        "    # cv2.imwrite('face.jpg', faces)"
      ],
      "metadata": {
        "id": "xm1gnldgNSGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "video_name = 'dzdbunuaag.mp4'\n",
        "\n",
        "cap = cv2.VideoCapture(os.path.join('Dataset', video_name))\n",
        "\n",
        "dimensions = (300, 300)\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "video_writer = cv2.VideoWriter(os.path.join('Output', 'face_'+video_name), cv2.VideoWriter_fourcc(*'DIVX'), fps, dimensions, isColor=False)\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('Models/haarcascade_frontalface_alt2.xml')\n",
        "\n",
        "\n",
        "for frame_i in range(0,frame_count, 10):\n",
        "  ret, frame = cap.read()\n",
        "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "  frame = exposure.equalize_hist(frame)\n",
        "  frame = img_as_ubyte(frame)\n",
        "  faces = face_cascade.detectMultiScale(frame, 1.1, 3)\n",
        "  if len(faces) == 1:\n",
        "    prev_face = faces\n",
        "  else:\n",
        "    faces = prev_face\n",
        "  x, y, w, h = faces[0]\n",
        "  width_fill = 300 - ((x + w) - x)\n",
        "  height_fill = 300 - ((y + h) - y)\n",
        "  face_frame = frame[y:y + h + height_fill, x:x + w + width_fill]\n",
        "\n",
        "  video_writer.write(face_frame)\n",
        "\n",
        "cap.release()\n",
        "video_writer.release()"
      ],
      "metadata": {
        "id": "A0NrPbWVGMNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_face_haar(cap, frame_skip):\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  face_cascade = cv2.CascadeClassifier('Models/haarcascade_frontalface_alt2.xml')\n",
        "  frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  p_frames = []\n",
        "  for frame_i in range(0, frame_count, frame_skip):\n",
        "    ret, frame = cap.read()\n",
        "    print(f'FRAME: {frame_i}')\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    faces = face_cascade.detectMultiScale(frame, 1.1, 4)\n",
        "    # print(len(faces[0]['box']))\n",
        "    print(faces)\n",
        "    if len(faces) == 4:\n",
        "      prev_face = faces\n",
        "    else:\n",
        "      try:\n",
        "        faces = prev_face\n",
        "      except:\n",
        "        while len(faces) < 1:\n",
        "          print(faces)\n",
        "          ret_next, frame_next = cap.read()\n",
        "          faces = face_cascade.detectMultiScale(frame, 1.1, 4)\n",
        "    print(faces)\n",
        "    x, y, w, h = faces[0]\n",
        "    width_fill = 300 - ((x + w + ((w - wl)/2)) - x)\n",
        "    height_fill = 300 - ((y + h + ((h - hl)/2)) - y)\n",
        "    p_frames.append(frame[y:y + h + height_fill, x:x + w + width_fill])\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_i-1)\n",
        "    xl, yl, wl, hl = x, y, w, h\n",
        "  return p_frames\n",
        "\n",
        "# video_name =\n",
        "print(glob.glob(os.path.join('Dataset', '*'))[13])\n",
        "video_name = glob.glob(os.path.join('Dataset', '*'))[0].split('/')[1]\n",
        "cap = cv2.VideoCapture(glob.glob(os.path.join('Dataset', '*'))[13])\n",
        "\n",
        "dimensions = (300, 300)\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "video_writer = cv2.VideoWriter(os.path.join('Output', 'face_'+video_name), cv2.VideoWriter_fourcc(*'DIVX'), fps, dimensions, isColor=False)\n",
        "\n",
        "# with tf.device('/device:GPU:0'):\n",
        "video_frames = crop_face_haar(cap, 10)\n",
        "print(video_frames)\n",
        "[video_writer.write(cv2.cvtColor(face_frame, cv2.COLOR_RGB2BGR)) for face_frame in video_frames]\n",
        "cap.release()\n",
        "video_writer.release()"
      ],
      "metadata": {
        "id": "kWh3iAlUQsKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y8YtywXoZANL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(glob.glob(os.path.join('Dataset', '*'))[13:])"
      ],
      "metadata": {
        "id": "yYL17qqQRf5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 - Crop only Faces with MTCNN\n",
        "\n",
        "The flickering got smoother, but it's still there. <br>\n",
        "Additionaly, seeking to save processing time, I've set the pre-processing to process only 6 of the 30 Frames per second, jumping from 5 to 5."
      ],
      "metadata": {
        "id": "BlcloBf01b8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_face(cap, frame_skip):\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  p_frames = []\n",
        "  for frame_i in range(0, frame_count, frame_skip):\n",
        "    ret, frame = cap.read()\n",
        "    print(f'FRAME: {frame_i}')\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame = exposure.equalize_hist(frame)\n",
        "    faces = f_detector.detect_faces(frame)\n",
        "    # print(len(faces[0]['box']))\n",
        "    print(faces)\n",
        "    if len(faces) >= 1:\n",
        "      prev_face = faces\n",
        "    else:\n",
        "      try:\n",
        "        faces = prev_face\n",
        "      except:\n",
        "        while len(faces) < 1:\n",
        "          print(faces)\n",
        "          ret_next, frame_next = cap.read()\n",
        "          faces = f_detector.detect_faces(frame_next)\n",
        "\n",
        "    x, y, w, h = faces[0]['box']\n",
        "    if frame_i == 0:\n",
        "      xl, yl, wl, hl = x, y, w, h\n",
        "    width_fill = 300 - (((x + w) - x) + ((xl + wl) - xl))//2\n",
        "    height_fill = 300 - (((y + h) - y) + ((yl + hl) - yl))//2\n",
        "    p_frames.append(frame[y:y + h + height_fill, x:x + w + width_fill])\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_i-1)\n",
        "    xl, yl, wl, hl = x, y, w, h\n",
        "  return p_frames\n",
        "\n",
        "\n",
        "for file_path in glob.glob(os.path.join('Dataset', '*'))[13:]:\n",
        "  print(file_path)\n",
        "  video_name = file_path.split('/')[1]\n",
        "\n",
        "  cap = cv2.VideoCapture(file_path)\n",
        "\n",
        "  dimensions = (300, 300)\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "  video_writer = cv2.VideoWriter(os.path.join('Output', 'mtcnn_face_'+video_name), cv2.VideoWriter_fourcc(*'DIVX'), fps, dimensions, isColor=True)\n",
        "\n",
        "  f_detector = MTCNN()\n",
        "\n",
        "  # with tf.device('/device:GPU:0'):\n",
        "  video_frames = crop_face(cap, 10)\n",
        "  [video_writer.write(cv2.cvtColor(face_frame, cv2.COLOR_RGB2BGR)) for face_frame in video_frames]\n",
        "  cap.release()\n",
        "  video_writer.release()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dQJK1jHr-O_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 - Face Crop MTCNN + Canny Edge Detector."
      ],
      "metadata": {
        "id": "el0bzte16lrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_writer = cv2.VideoWriter(os.path.join('Output', 'mtcnn_face_edges_'+video_name), cv2.VideoWriter_fourcc(*'DIVX'), fps, (300, 300), isColor=False)\n",
        "\n",
        "video_edges = [cv2.Canny(face_frame, threshold1=30, threshold2=40) for face_frame in video_frames]\n",
        "[video_writer.write(face_edge) for face_edge in video_edges]\n",
        "\n",
        "cap.release()\n",
        "video_writer.release()\n",
        "\n",
        "f, axarr = plt.subplots(1,2)\n",
        "axarr[0].imshow(video_frames[0])\n",
        "axarr[1].imshow(video_edges[0])\n",
        "axarr[0].axis('off')\n",
        "axarr[1].axis('off')\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "dO5UtzVV2Qgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2 - Face Crop MTCNN + Canny Edge Detector + Gaussian Blur."
      ],
      "metadata": {
        "id": "V5ib-zCyNjfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_writer = cv2.VideoWriter(os.path.join('Output', 'mtcnn_face_edges_gaublur_'+video_name), cv2.VideoWriter_fourcc(*'DIVX'), fps, (300, 300), isColor=False)\n",
        "\n",
        "blur_video = [cv2.GaussianBlur(face_frame, (5,5), 0) for face_frame in video_frames]\n",
        "video_edges = [cv2.Canny(face_frame, threshold1=30, threshold2=40) for face_frame in blur_video]\n",
        "[video_writer.write(face_edge) for face_edge in video_edges]\n",
        "\n",
        "cap.release()\n",
        "video_writer.release()\n",
        "\n",
        "f, axarr = plt.subplots(1,2)\n",
        "axarr[0].imshow(video_frames[0])\n",
        "axarr[1].imshow(video_edges[0])\n",
        "axarr[0].axis('off')\n",
        "axarr[1].axis('off')\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "Z8QV0kSx6wHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2 - Face Crop MTCNN + Canny Edge Detector + Gaussian Blur + Gray Scale."
      ],
      "metadata": {
        "id": "Gk_u_lmTPS69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_writer = cv2.VideoWriter(os.path.join('Output', 'mtcnn_face_edges_gaublur_gray'+video_name), cv2.VideoWriter_fourcc(*'DIVX'), fps, (300, 300), isColor=False)\n",
        "\n",
        "grey_video = [cv2.cvtColor(face_frame, cv2.COLOR_BGR2GRAY) for face_frame in video_frames]\n",
        "blur_video = [cv2.GaussianBlur(face_frame, (5,5), 0) for face_frame in video_frames]\n",
        "video_edges = [cv2.Canny(face_frame, threshold1=30, threshold2=40) for face_frame in blur_video]\n",
        "\n",
        "[video_writer.write(face_edge) for face_edge in video_edges]\n",
        "cap.release()\n",
        "video_writer.release()\n",
        "\n",
        "f, axarr = plt.subplots(1,2)\n",
        "axarr[0].imshow(video_frames[0])\n",
        "axarr[1].imshow(video_edges[0])\n",
        "axarr[0].axis('off')\n",
        "axarr[1].axis('off')\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "w5_3lEjYNow-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- SIFT Detection\n",
        "\n",
        "Applying SIFT Detection to identify points of interest of the image"
      ],
      "metadata": {
        "id": "IpvMCHQiTYgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SIFT(img):\n",
        "  sift_detector = cv2.SIFT_create(nfeatures=2000)\n",
        "  final_img = img.copy()\n",
        "  keypoints = sift_detector.detect(final_img)\n",
        "  final_img = cv2.drawKeypoints(final_img, keypoints, final_img, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "  return final_img\n",
        "\n",
        "\n",
        "video_SIFT = [SIFT(face_frame) for face_frame in video_frames.copy()]\n",
        "\n",
        "f, axarr = plt.subplots(1,2)\n",
        "axarr[0].imshow(video_frames[0])\n",
        "axarr[1].imshow(video_edges[0])\n",
        "axarr[0].axis('off')\n",
        "axarr[1].axis('off')\n",
        "plt.plot()\n"
      ],
      "metadata": {
        "id": "1RGL84OpTkoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5- Color Histogram"
      ],
      "metadata": {
        "id": "ZJIFtqaoOMx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "Wgw6SUzESSY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_color_hist(img):\n",
        "  channels = [Counter(img[:,:,i].reshape(-1)) for i in range(3)]\n",
        "  hist = {i:[channels[i][intensity] for intensity in range(256)] for i in range(3)}\n",
        "\n",
        "  return hist\n",
        "\n",
        "plt.plot(create_color_hist(video_frames[0])[0], color='red')\n",
        "plt.plot(create_color_hist(video_frames[0])[1], color='green')\n",
        "plt.plot(create_color_hist(video_frames[0])[2], color='blue')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hFZ-WWR-PABz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prev = create_color_hist(video_frames[1])\n",
        "next = create_color_hist(video_frames[2])\n",
        "\n",
        "diff = next.copy()\n",
        "# print(diff)\n",
        "for key in prev:\n",
        "  diff[key] = np.array(next[key]) - np.array(prev[key])"
      ],
      "metadata": {
        "id": "DgAwkAWRWfNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(diff[0], color='red')\n",
        "plt.plot(diff[1], color='green')\n",
        "plt.plot(diff[2], color='blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UEQr6H4GWuI2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}